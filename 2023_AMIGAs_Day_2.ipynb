{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOykKJQkHiuZWBRiF6jTXMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjcook13/AMIGAS_2023/blob/main/2023_AMIGAs_Day_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/IPAMlogo.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n"
      ],
      "metadata": {
        "id": "39cvxqXBg3AU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Driven Mathematical Models and Simulation Techniques**\n",
        "\n",
        "**Instructor:** Dr. Keisha Cook\n",
        "\n",
        "**Affiliation:** Clemson University\n",
        "\n",
        "**Email:** keisha@clemson.edu\n"
      ],
      "metadata": {
        "id": "0tuCnxdleMwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial Summary\n",
        "\n",
        "Applied mathematics research involves problems that rely on samples of data taken from the outside world. This can be seen in fields ranging from biology, physics, engineering, environmental science, and more. To understand and predict information about data that has yet to be collected, we rely on simulations of real world scenarios. We can use the collected data to compute known parameters. The relevant parameters can be used to develop models that simulate the behavior of the collected data. To predict unknown parameters and future outcomes of our scenarios, we rely on inference methods. In this short course, we will learn how to build a mathematical model from data, simulate data that closely represents the collected data, and use the simulations to make model predictions. Overall, we want the data to influence our model development decisions and for the information we learn from the models to help us understand the data."
      ],
      "metadata": {
        "id": "7ZqcAya-eqkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make an R Colab Notebook**\n",
        "\n",
        "Runtime -> Change runtime type -> Select \"R\" from pull-down menu -> Press save\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/RColabRuntime.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n"
      ],
      "metadata": {
        "id": "wSG0Ee3MnSvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Packages**"
      ],
      "metadata": {
        "id": "XY9yg68xocpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"ggplot2\")\n",
        "install.packages(\"gridExtra\")\n",
        "install.packages(\"shiny\")\n",
        "install.packages(\"ggdist\")\n",
        "install.packages(\"KernSmooth\")\n",
        "install.packages(\"gtools\")\n",
        "install.packages(\"gridExtra\")\n",
        "install.packages(\"expm\")\n",
        "install.packages(\"forecast\")\n",
        "install.packages(\"dplyr\")\n",
        "install.packages(\"sde\")"
      ],
      "metadata": {
        "id": "La5D0ctwoiQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Sampling from an Unknown Probability Distribution\n",
        "\n",
        "We want to sample a value from an unknown discrete or continuous distribution.\n",
        "\n",
        "Steps:\n",
        "1.   Define the CDF.\n",
        "2.   Generate a Uniform random variable $r$ between 0 and 1. I.e. $r = U(0,1)$.\n",
        "3.   Find the point $F_X(x)=r$ and solve for $x$.\n",
        "\n",
        "Formally in a pseudocode,\n",
        "\n",
        "$$\n",
        "F_R(r) = \\left\\{  \\begin{array}{ll}\n",
        "0\\text{ for }r<0,\\\\\n",
        "r\\text{ for }0\\leq r \\leq 1,\\\\\n",
        "1\\text{ for }r>1  \n",
        " \\end{array}\\right\\}.\n",
        "$$\n",
        "\n",
        "If we know the cumulative distribution function, $x\\sim F_X(x)$, we can invert it and use $r$ to generate a sample for $x$ according to:\n",
        "\n",
        "$$\n",
        "x = F_X^{-1}(F_R(r)) = F_X^{-1}(r)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "rOlSCcZdfkGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example:**\n",
        "\n",
        "For the continuous valued __Exponential__ distribution, we saw that:\n",
        "$$\n",
        "f_X(x) = a\\exp(-ax)\\text{, for }x\\ge 0,\n",
        "$$\n",
        "which we can integrate with respect to $x$ to find:\n",
        "$$\n",
        "F_X(x) = 1-\\exp(-ax)\\text{, for }x\\ge 0.\n",
        "$$\n",
        "Since we know for the uniform random variable $r = F_R(r) = F_X(x)$, we can now invert to solve for $x$ provided that we can generate $r$:\n",
        "$$\n",
        "x = F_X^{-1}(r) = -\\frac{\\log(1-r)}{a},\n",
        "$$\n",
        "or since $(1-r)$ has the same distribution as $r$, we can simplify to:\n",
        "$$\n",
        "x = F_X^{-1}(r) = -\\frac{\\log(r)}{a},\n",
        "$$\n",
        "\n",
        "The code is below."
      ],
      "metadata": {
        "id": "9uPMsD4ixbbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(ggplot2)\n",
        "library(ggdist)\n",
        "\n",
        "a <- 1.0  # Parameter for exponential distribution\n",
        "NS <- 10000  # Number of samples\n",
        "\n",
        "#let r ~ U(0,1)\n",
        "r = runif(NS,0,1)\n",
        "x = -log(r)/a\n",
        "\n",
        "# Generate random variables from exponential distribution\n",
        "x_true <- rexp(NS, rate = a)\n",
        "plot(density(x_true))\n",
        "\n",
        "# Plot the distribution of sampled variables\n",
        "p <- ggplot(data.frame(x), aes(x = x)) +\n",
        "  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = \"lightblue\", color = \"black\") +\n",
        "  geom_density(aes(y = after_stat(density)), color = \"red\", size = 1) +\n",
        "  labs(x = \"Value of random variable\", y = \"Probability Density\") +\n",
        "  ggtitle(\"Distribution of Sampled Variables\")\n",
        "\n",
        "print(p)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lflDPUYNxWbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rejection sampling\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Rejection sampling is a general technique that can be used when you have a function that majorizes your unknown distribution. It involves sampling from a simpler distribution (e.g., uniform or Gaussian) and then accepting or rejecting samples based on a comparison between the target distribution and the majorizing function.\n",
        "\n",
        "The unknown target distribution given the equation of a triangle\n",
        "$$ \\frac{10-|x-5|}{12}$$ defined on $[2,8]$.\n",
        "\n",
        "At every iteration we,\n",
        "1.   Generate a uniform random variable from $[0,10]$.\n",
        "2.   Generate another uniform random variable from $[0,30]$.\n",
        "3.   If the 2nd random variable is less than or equal to the target distribution, we accept the value and append it to our list.\n",
        "4. If the 2nd random variable is greater than the target distribution, we reject the value and return to step 1.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MXnOGSIDfozi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the unknown target distribution function\n",
        "target_distribution <- function(x) {\n",
        "  if (x >= 2 && x <= 8) {\n",
        "    return((10 - abs(x - 5)) / 12)\n",
        "  } else {\n",
        "    return(0)\n",
        "  }\n",
        "}\n",
        "\n",
        "# Define the majorizing function (uniform distribution)\n",
        "majorizing_function <- function(x) {\n",
        "  #return(1/10)  # Uniform distribution within [0, 10]\n",
        "  return(1/30)  # Uniform distribution within [0, 30]\n",
        "}\n",
        "\n",
        "# Define the number of samples to generate\n",
        "num_samples <- 1000\n",
        "\n",
        "# Generate samples using rejection sampling\n",
        "samples <- c()\n",
        "while (length(samples) < num_samples) {\n",
        "  x <- runif(1, 0, 10)\n",
        "  y <- runif(1, 0, majorizing_function(x))\n",
        "  if (y <= target_distribution(x)) {\n",
        "    samples <- c(samples, x)\n",
        "  }\n",
        "}\n",
        "\n",
        "# Plot the target distribution and the sampled points\n",
        "x_values <- seq(0, 10, length.out = 1000)\n",
        "target_values <- sapply(x_values, target_distribution)\n",
        "\n",
        "plot(x_values, target_values, type = \"l\", lwd = 2, col = \"blue\", xlab = \"x\", ylab = \"Density\",\n",
        "     main = \"Rejection Sampling Example\")\n",
        "hist(samples, breaks = 30, freq = FALSE, add = TRUE, col = \"lightblue\", border = \"black\", alpha = 0.5)\n",
        "legend(\"topright\", legend = c(\"Target Distribution\", \"Sampled Points\"), lwd = c(2, 0), col = c(\"blue\", \"lightblue\"), fill = c(\"blue\", \"lightblue\"))\n"
      ],
      "metadata": {
        "id": "_AR2zrpW1ljH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram above shows the accepted values.\n",
        "\n",
        "Keep in mind that the effectiveness of rejection sampling depends on finding an appropriate majorizing function that bounds the target distribution. In practice, it might require some trial and error to find the best majorizing function for a given target distribution."
      ],
      "metadata": {
        "id": "OBriGwJe1tCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unknown target distribution is distributed as two wells which can be represented as exponential distributions.\n",
        "\n",
        "Well 1: $e^{-(x-2)^2}$\n",
        "\n",
        "Well 2: $e^{-(x-8)^2}$\n",
        "\n",
        "Thus the distribution as a whole can be expressed as the sum of these two wells.\n",
        "\n",
        "At every iteration we,\n",
        "1.   Generate a uniform random variable from $[0,10]$.\n",
        "2.   Generate another uniform random variable from $[0,1]$.\n",
        "3.   If the 2nd random variable is less than or equal to the target distribution, we accept the value and append it to our list.\n",
        "4. If the 2nd random variable is greater than the target distribution, we reject the value and return to step 1."
      ],
      "metadata": {
        "id": "ZhSDru-01v1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the unknown target distribution function with two wells\n",
        "target_distribution <- function(x) {\n",
        "  well1 <- exp(-(x - 2) ^ 2)\n",
        "  well2 <- exp(-(x - 8) ^ 2)\n",
        "  return(well1 + well2)\n",
        "}\n",
        "\n",
        "# Define the majorizing function (uniform distribution)\n",
        "majorizing_function <- function(x) {\n",
        "  return(1)  # Uniform distribution within the range\n",
        "}\n",
        "\n",
        "# Define the number of samples to generate\n",
        "num_samples <- 1000\n",
        "\n",
        "# Generate samples using rejection sampling\n",
        "samples <- c()\n",
        "while (length(samples) < num_samples) {\n",
        "  x <- runif(1, 0, 10)\n",
        "  y <- runif(1, 0, majorizing_function(x))\n",
        "  if (y <= target_distribution(x)) {\n",
        "    samples <- c(samples, x)\n",
        "  }\n",
        "}\n",
        "\n",
        "# Plot the target distribution and the sampled points\n",
        "x_values <- seq(0, 10, length.out = 1000)\n",
        "target_values <- sapply(x_values, target_distribution)\n",
        "\n",
        "plot(x_values, target_values, type = \"l\", lwd = 2, col = \"blue\", xlab = \"x\", ylab = \"Density\",\n",
        "     main = \"Rejection Sampling Example with Two Wells\")\n",
        "hist(samples, breaks = 30, freq = FALSE, add = TRUE, col = \"lightblue\", border = \"black\", alpha = 0.5)\n",
        "legend(\"topright\", legend = c(\"Target Distribution\", \"Sampled Points\"), lwd = c(2, 0), col = c(\"blue\", \"lightblue\"), fill = c(\"blue\", \"lightblue\"))\n"
      ],
      "metadata": {
        "id": "m0Bly0Gu2L16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel Density Estimation\n",
        "\n",
        "If you have a set of observations from the unknown distribution, you can estimate its probability density function (PDF) using kernel density estimation. Once you have the estimated PDF, you can sample from it using techniques like random sampling or Markov Chain Monte Carlo (MCMC) methods.\n",
        "\n",
        "We generate a random sample of size 1000 from a known distribution (in this case, a Gaussian distribution with mean 2 and standard deviation 1.5). This sample will serve as our data.\n",
        "\n",
        "Next, we use the \"gaussian_kde\" function from SciPy to perform the kernel density estimation. This function estimates the probability density function (PDF) using a Gaussian kernel. The Gaussian kernel is produced by sampling points from the continuous Gaussian. By default, it automatically determines the bandwidth parameter for the kernel based on the sample.\n",
        "\n",
        "We generate a set of x-values using \"np.linspace\" to define the range over which we want to evaluate the density estimate.\n",
        "\n",
        "Then, we evaluate the kernel density estimate at the x-values using the evaluate method of the KDE object."
      ],
      "metadata": {
        "id": "zrdR7BJwfyRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(ggplot2)\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "set.seed(0)\n",
        "\n",
        "# Generate a random sample from a known distribution (e.g., Gaussian)\n",
        "sample <- rnorm(1000, mean = 2, sd = 1.5)\n",
        "\n",
        "# Perform kernel density estimation\n",
        "kde <- density(sample)\n",
        "\n",
        "# Generate x-values for evaluation\n",
        "x_values <- seq(-5, 10, length.out = 1000)\n",
        "\n",
        "# Evaluate the kernel density estimate at the x-values\n",
        "density_estimation <- approxfun(kde)(x_values)\n",
        "\n",
        "# Plot the original sample and the kernel density estimate\n",
        "p <- ggplot(data.frame(x = sample), aes(x = x)) +\n",
        "  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.5) +\n",
        "  geom_line(data = data.frame(x = x_values, density = density_estimation), aes(x = x, y = density), color = \"red\", size = 1) +\n",
        "  labs(x = \"x\", y = \"Density\") +\n",
        "  ggtitle(\"Kernel Density Estimation Example\")\n",
        "\n",
        "print(p)\n"
      ],
      "metadata": {
        "id": "-WGMQNGg2p5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we generate two random samples from different distributions. We will learn to use KDE to estimate the densities of multiple samples drawn from different distributions.\n",
        "\n",
        "*   Sample 1 is drawn from a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "*   Sample 2 is drawn from a Gaussian distribution with mean 4 and standard deviation 0.5.\n",
        "\n",
        "We then concatenate the two samples into a single array called \"combined_sample\".\n",
        "\n",
        "Next, we perform kernel density estimation using the \"gaussian_kde\" function from SciPy on the combined sample.\n",
        "\n",
        "We generate a set of x-values using \"np.linspace\" to define the range over which we want to evaluate the density estimate. Then, we evaluate the kernel density estimate at the x-values using the evaluate method of the KDE object."
      ],
      "metadata": {
        "id": "F0E_eYFE4iPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(ggplot2)\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "set.seed(0)\n",
        "\n",
        "# Generate two random samples from different distributions\n",
        "sample1 <- rnorm(500, mean = 0, sd = 1)\n",
        "sample2 <- rnorm(300, mean = 4, sd = 0.5)\n",
        "\n",
        "# Concatenate the two samples\n",
        "combined_sample <- c(sample1, sample2)\n",
        "\n",
        "# Perform kernel density estimation\n",
        "kde <- density(combined_sample)\n",
        "\n",
        "# Generate x-values for evaluation\n",
        "x_values <- seq(-2, 6, length.out = 1000)\n",
        "\n",
        "# Evaluate the kernel density estimate at the x-values\n",
        "density_estimation <- approxfun(kde$x, kde$y)(x_values)\n",
        "\n",
        "# Plot the original samples and the kernel density estimate\n",
        "p <- ggplot() +\n",
        "  geom_histogram(data = data.frame(x = sample1), aes(x = x), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.5) +\n",
        "  geom_histogram(data = data.frame(x = sample2), aes(x = x), bins = 30, fill = \"lightgreen\", color = \"black\", alpha = 0.5) +\n",
        "  geom_line(data = data.frame(x = x_values, density = 100*density_estimation), aes(x = x, y = density), color = \"red\", size = 1) +\n",
        "  labs(x = \"x\", y = \"Density\") +\n",
        "  ggtitle(\"Kernel Density Estimation Example\") +\n",
        "  scale_fill_manual(values = c(\"lightblue\", \"lightgreen\"), guide = \"none\")\n",
        "\n",
        "print(p)"
      ],
      "metadata": {
        "id": "cjNTFZz44jKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beta Distribution\n",
        "\n",
        "The **CONTINUOUS** Beta distribution is used to model the behavior or random variables thaht fall in the interval $(0,1)$. The probability density function (PDF) is defined as\n",
        "\n",
        "$$f(x) = \\frac{Γ(\\alpha + \\beta)}{Γ(\\alpha)Γ(\\beta)}x^{\\alpha - 1}(1-x)^{\\beta -1}$$ where $0 < x < 1$.\n",
        "\n",
        "The **mean** of a Beta random variable is $\\frac{\\alpha}{\\alpha + \\beta}$.\n",
        "\n",
        "The **variance** of a Beta random variable is $\\frac{\\alpha \\beta}{(\\alpha+\\beta+1)(\\alpha+\\beta)^2}$."
      ],
      "metadata": {
        "id": "pJz_zBvxvTxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(ggplot2)\n",
        "\n",
        "# Parameters of the Beta distribution\n",
        "alpha <- 2   # Shape parameter alpha\n",
        "beta <- 5    # Shape parameter beta\n",
        "\n",
        "# Generate x values\n",
        "x <- seq(0, 1, length.out = 100)\n",
        "\n",
        "# Calculate y values using the Beta distribution\n",
        "y <- dbeta(x, alpha, beta)\n",
        "\n",
        "# Create the plot\n",
        "ggplot() +\n",
        "  geom_line(aes(x, y), color = \"blue\") +\n",
        "  xlim(0, 1) +\n",
        "  ylim(0, max(y) + 0.1) +\n",
        "  xlab(\"Probability\") +\n",
        "  ylab(\"Density\") +\n",
        "  ggtitle(\"Beta Distribution\")\n"
      ],
      "metadata": {
        "id": "CE_Gk68Er_Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional Probability\n",
        "\n",
        "What is the probability of an event given a specific set of information?\n",
        "\n",
        "**Law of Total Probability**\n",
        "\n",
        "Suppose that $B_1, \\dots, B_n$ is a partition of the sample space $S$, then for any event $A$ we have\n",
        "$$P(A) = \\sum_{i=1}^{n}P(A \\cap B_i) = \\sum_{i=1}^{n}P(A|B_i)P(B_i)$$\n",
        "\n",
        "i.e.\n",
        "$$P(A) = P(A \\cap B_1) + P(A \\cap B_2) + \\dots$$\n",
        "\n",
        "$$= P(B_1)P(A|B_1) + P(B_2)P(A|B_2) + \\dots$$\n",
        "\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/TotalProb.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "KLSSFl0be77R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prior Probability**\n",
        "The probability of an event before the collection of new data.\n",
        "\n",
        "**Posterior Probability**\n",
        "The probability of an event after the collection of new data.\n",
        "\n",
        "**Bayes' Formula**\n",
        "The posterior probability or the probability of event $A$ occurring given that event $B$ has occurred.\n",
        "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A)P(B|A)}{P(B)}$$\n",
        "\n",
        "Note that the marginal probability (denominator) is the probability of the data with all possible hypotheses. That is $P(B|A)P(A) + P(B|A^c)P(A^c)$. In general,\n",
        "\n",
        "$$P(A_i|B) = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{n}P(B|A_j)P(A_j)} $$\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/bayes.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n"
      ],
      "metadata": {
        "id": "JzUqpDGekW5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prior probabilities\n",
        "prior_A <- 0.3   # Prior probability of event A\n",
        "prior_B <- 0.7   # Prior probability of event B\n",
        "\n",
        "# Conditional probabilities\n",
        "p_B_given_A <- 0.6   # Probability of event B given event A\n",
        "p_B_given_notA <- 0.2   # Probability of event B given not event A\n",
        "\n",
        "# Bayes' theorem calculation\n",
        "p_A_given_B <- (p_B_given_A * prior_A) / ((p_B_given_A * prior_A) + (p_B_given_notA * prior_B))\n",
        "\n",
        "# Output the result\n",
        "cat(\"Probability of A given B:\", p_A_given_B, \"\\n\")\n"
      ],
      "metadata": {
        "id": "Y_y4nQnZQuzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Framework\n",
        "\n",
        "Let $\\theta$ be some unknown parameter (random variable); typically defined $\\theta = (\\theta_1, \\theta_2, \\dots, \\theta_r)$, where $r$ denotes the number of parameters in the model.\n",
        "\n",
        "Let $y = (y_1, y_2, \\dots, y_n)$ be a **data** vector which provides information about $\\theta$.\n",
        "\n",
        "Let $f(y|\\theta)$ be the **sampling distribution** of $y$ given $\\theta$.\n",
        "\n",
        "Let $p(\\theta)$ be the **prior distribution** on $\\theta$ representing the degree of belief of $\\theta$.\n",
        "\n",
        "We define the **posterior distribution** to be $p(\\theta|y)$, the updated knowledge about $\\theta$ conditional on $y$.\n",
        "\n",
        "In this context, we redefine the **Bayes Theorem** as\n",
        "$$p(\\theta|y) \\sim f(y|\\theta)p(\\theta)$$\n",
        "\n",
        "The complete formula:\n",
        "$$p(\\theta|y) = \\frac{f(y|\\theta)p(\\theta)}{∫f(y|\\theta)p(\\theta)}$$\n",
        "\n",
        "\n",
        "Here $\\theta$ is quantity that we wish to infer, and $y$ is our data. Bayes' Theorem tells us how observing $y$ affects our knowledge of the true quantity $\\theta$, based on our current knowledge of $\\theta$ represented by the prior $p(\\theta)$, and our understanding of the model, represented by the likelihood $p(y \\, | \\, \\theta)$.\n",
        "\n",
        "**Additional Background:**\n",
        "The normalizing constant in Bayes' Theorem is required to ensure that the posterior probabilities over $\\theta$ sum to $1$. It can be explicitly written as\n",
        "\n",
        "$$ p(y \\, | \\, \\theta) = \\int p(y \\, | \\, \\theta) \\, p(\\theta) \\, d\\theta $$\n",
        "\n",
        "This is an integral over *all* possible values of $\\theta$."
      ],
      "metadata": {
        "id": "8IBo92gsSp6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on Binomial Model\n",
        "\n",
        "Let $y$ be the number of successes over $n$ trials; $y = 0,1,2, \\dots, n$. Let $\\theta$ be the probability of success for a single trial; $0 < \\theta < 1$.\n",
        "\n",
        "We need to define a **prior** distribution for $\\theta$. We can let this be uniformative $p(\\theta)=1$ (assume we know nothing about the parameter), we can assume $\\theta$ follows some probability distribution. For example, we could say that $\\theta \\sim U(0,1)$ or $\\theta \\sim Beta(a,b)$ when $\\theta$ is non-informative with $a=b=1$.\n",
        "\n",
        "Compute the **posterior distribution** using the definition above.\n",
        "\n",
        "$$p(\\theta|y) = {n \\choose y}\\theta^y(1-\\theta)^{n-y} \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{Β(a,b)}$$\n",
        "\n",
        "Factor out the constants and we have\n",
        "\n",
        "$$p(\\theta|y) = \\theta^{y+1-1}(1-\\theta)^{n-y+1-1}$$\n",
        "\n",
        "This is in the form of a Beta probability distribution\n",
        "\n",
        "$$Beta(y+1,n-y+1)$$\n",
        "\n",
        "From our study of the Beta distribution, the $E(\\theta|y) = \\frac{y+1}{n-y+1}$\n",
        "\n"
      ],
      "metadata": {
        "id": "2JqHw5HNZ5ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required packages\n",
        "library(ggplot2)\n",
        "library(gridExtra)\n",
        "\n",
        "# Generate some example data\n",
        "# One experiment of the number of successes in n trials\n",
        "set.seed(123)\n",
        "n <- 100   # Number of trials\n",
        "p_true <- 0.3  # True probability of success\n",
        "observed_success <- rbinom(1, n, p_true)  #this is y in our example above\n",
        "\n",
        "cat(\"Number of successes:\", observed_success, \"\\n\")\n",
        "\n",
        "# Prior parameters\n",
        "# note that this is essentially Uniform whem alpha = beta = 1\n",
        "alpha <- 1  # Shape parameter of the Beta prior\n",
        "beta <- 1   # Shape parameter of the Beta prior\n",
        "\n",
        "# Bayesian inference\n",
        "posterior_alpha <- alpha + observed_success\n",
        "posterior_beta <- beta + n - observed_success\n",
        "\n",
        "# Generate posterior samples\n",
        "num_samples <- 10000\n",
        "posterior_samples <- rbeta(num_samples, posterior_alpha, posterior_beta)\n",
        "\n",
        "# Plotting the posterior distribution\n",
        "posterior_density <- density(posterior_samples)\n",
        "posterior_df <- data.frame(x = posterior_density$x, y = posterior_density$y)\n",
        "prior_df <- data.frame(x = c(0, 1), y = dbeta(c(0, 1), alpha, beta))\n",
        "\n",
        "p <- ggplot() +\n",
        "  geom_line(data = prior_df, aes(x, y), color = \"blue\", linetype = \"dashed\") +\n",
        "  geom_line(data = posterior_df, aes(x, y), color = \"red\") +\n",
        "  xlim(0, 1) +\n",
        "  ylim(0, max(posterior_df$y, prior_df$y)) +\n",
        "  xlab(\"Probability of Success\") +\n",
        "  ylab(\"Density\") +\n",
        "  ggtitle(\"Posterior Distribution\")\n",
        "\n",
        "print(p)\n",
        "\n",
        "# Plotting the prior and posterior distributions together\n",
        "prior_plot <- ggplot() +\n",
        "  geom_line(data = prior_df, aes(x, y), color = \"blue\", linetype = \"dashed\") +\n",
        "  xlim(0, 1) +\n",
        "  ylim(0, max(prior_df$y)) +\n",
        "  xlab(\"Probability of Success\") +\n",
        "  ylab(\"Density\") +\n",
        "  ggtitle(\"Prior Distribution\")\n",
        "\n",
        "posterior_plot <- ggplot() +\n",
        "  geom_line(data = posterior_df, aes(x, y), color = \"red\") +\n",
        "  xlim(0, 1) +\n",
        "  ylim(0, max(posterior_df$y)) +\n",
        "  xlab(\"Probability of Success\") +\n",
        "  ylab(\"Density\") +\n",
        "  ggtitle(\"Posterior Distribution\")\n",
        "\n",
        "grid.arrange(prior_plot, posterior_plot, nrow = 2)\n",
        "\n",
        "# Summary statistics\n",
        "mean_posterior <- mean(posterior_samples)\n",
        "median_posterior <- median(posterior_samples)\n",
        "credible_interval <- quantile(posterior_samples, c(0.025, 0.975))\n",
        "\n",
        "cat(\"Posterior mean:\", mean_posterior, \"\\n\")\n",
        "cat(\"Posterior median:\", median_posterior, \"\\n\")\n",
        "cat(\"95% Credible Interval:\", credible_interval[1], \"-\", credible_interval[2], \"\\n\")\n"
      ],
      "metadata": {
        "id": "IuLeY09IkAYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on Normal Model\n",
        "\n",
        "Let $y$ be a normal random varible representing lengths. We know from our studies that the Normal distribution is defined as\n",
        "$$y|\\theta,\\sigma^2 \\sim N(\\theta,\\sigma^2).$$\n",
        "\n",
        "The parameters of interest are $\\theta$ the true average length and $\\sigma^2$ the true variance in length. The probability density function (PDF) is\n",
        "\n",
        "$$p(y|\\theta,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\{-\\frac{(y-\\theta)^2}{2\\sigma^2}\\}$$\n",
        "\n",
        "where $-\\infty < y < \\infty$.\n",
        "\n",
        "Suppose we have $n=200$ length observations. Each is conditionally independent:\n",
        "\n",
        "$$y_1, \\dots, y_n| \\theta, \\sigma^2 \\sim N(\\theta,\\sigma^2)$$\n",
        "\n",
        "Thus, $Y = (y_1, \\dots, y_n)$, then\n",
        "$$p(Y|\\theta, \\sigma^2 )= \\Pi_{i=1}^{n} p(y_i|\\theta, \\sigma^2 )$$\n",
        "\n",
        "$$= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\{-\\frac{(y_i-\\theta)^2}{2\\sigma^2}\\}$$\n",
        "\n",
        "$$=(2\\pi \\sigma^2)^{-\\frac{n}{2}}\\exp\\{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}(y_i - \\theta)^2\\}$$\n",
        "\n",
        "Assume one parameter is known, and compute the posterior for the other parameter.\n",
        "\n",
        "When $\\theta$ is unknown and $\\sigma^2$ is known, we fix $\\sigma^2$:\n",
        "\n",
        "$$p(\\theta|y,\\sigma^2) \\sim p(y|\\theta,\\sigma^2)p(\\theta)$$\n",
        "\n",
        "When $\\theta$ is known and $\\sigma^2$ is unknown, we fix $\\theta$:\n",
        "\n",
        "$$p(\\sigma^2|y,\\theta) \\sim p(y|\\theta, \\sigma^2)p(\\sigma^2)$$\n",
        "\n",
        "Assume both parameters are unknown, and compute a joint posterior for $(\\theta, \\sigma^2)$:\n",
        "\n",
        "$$p(\\theta,\\sigma^2|y) \\sim p(y|\\theta,\\sigma^2)p(\\theta,\\sigma^2)$$\n",
        "\n",
        "**Prior Model for $\\theta$**\n",
        "\n",
        "Assume that the true variance of $Y$ is $\\sigma^2 = 45$. Therefore, we need to define a prior for $\\theta$.\n",
        "$$\\theta \\sim N(\\mu_0, \\tau_0^2)$$, where $\\mu_0$ is the mean of the prior beliefs and $\\tau_0^2$ is the uncertainty in the prior beliefs. Thus the probability density function for the prior is\n",
        "$$p(\\theta) = \\frac{1}{\\sqrt{2\\pi \\tau_0^2}}\\exp \\{- \\frac{(\\theta - \\mu_0)^2}{2 \\tau_0^2}\\}$$\n",
        "\n",
        "We use Bayes Theorem to compute the posterior.\n",
        "\n",
        "$$p(\\theta| Y,\\sigma^2) = p(Y|\\theta,\\sigma^2)p(\\theta)$$\n",
        "\n",
        "$$= p(\\theta) p(y_1|\\theta,\\sigma^2)× … × p(y_n|\\theta,\\sigma^2)$$\n",
        "\n",
        "$$ = \\frac{1}{\\sqrt{2\\pi \\tau_0^2}} \\exp\\{ - \\frac{(\\theta - \\mu_0)^2}{2 \\tau_0^2}\\} × (2\\pi \\sigma^2)^{-\\frac{n}{2}} \\exp \\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\theta)^2\\}$$\n",
        "\n",
        "Factor out the constants and simplify the remaining terms by first expanding.\n",
        "\n",
        "$$\\sim \\exp \\{ -\\frac{1}{2} [(\\frac{\\theta - \\mu_0}{\\tau_0})^2 + \\sum_{i=1}^n (\\frac{y_i - \\theta}{\\sigma})^2] \\}$$\n",
        "\n",
        "Expand and combine the terms with $\\theta^2$, $\\theta$, and the remaining terms.\n",
        "\n",
        "Terms with $\\theta^2$:\n",
        "$$(\\frac{1}{\\tau_0^2}+\\frac{n}{\\sigma^2})\\theta^2$$\n",
        "\n",
        "Terms involving $\\theta$:\n",
        "$$-2(\\frac{\\mu_0}{\\tau_0^2} + \\frac{\\sum y_i}{\\sigma^2})\\theta$$\n",
        "\n",
        "Remaining terms:\n",
        "$$\\frac{\\mu_0^2}{\\tau_0^2}+ \\sum \\frac{x_i^2}{\\sigma^2}$$\n",
        "\n",
        "We then rewrite in the form of the normal distribution. Note that everything inside the exponential function is in the form of a quadratic equation. By completing the square, we can write this in the form of a Normal distribution using some algebra.\n",
        "\n",
        "$$\\sim \\exp \\{ (\\frac{\\theta - \\mu_0^*}{\\tau_0^*})^2 + (\\frac{\\mu_0^2}{\\tau^2}+ \\sum_i \\frac{y_i^2}{\\sigma^2} - \\frac{(\\frac{\\mu_0}{\\tau_0^2} + \\frac{\\sum y_i}{\\sigma^2})^2}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}) \\}$$\n",
        "\n",
        "Therefore, the posterior mean is\n",
        "\n",
        "$$\\mu^*_0 = \\frac{\\frac{\\mu_0}{\\tau_0^2} + \\frac{\\sum y_i}{\\sigma^2}}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}$$\n",
        "\n",
        "and the posterior variance is\n",
        "\n",
        "$$(\\tau^*_0)^2 = \\frac{1}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}$$\n",
        "\n",
        "The posterior distribution for $\\theta$ is Normal($\\mu^*_0,\\tau^*_0$).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BCpyJejg63Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required packages\n",
        "library(ggplot2)\n",
        "\n",
        "# Generate some example data\n",
        "set.seed(123)\n",
        "n <- 100   # Number of data points\n",
        "observed_data <- rnorm(n, mean = 5, sd = 2)  # Simulated observed data\n",
        "\n",
        "plot(density(observed_data))\n",
        "\n",
        "# Prior parameters\n",
        "prior_mean <- 0   # Prior mean\n",
        "# change the prior sd to something very small vs very large to see how this affects the posterior\n",
        "prior_sd <- 10    # Prior standard deviation\n",
        "\n",
        "# Bayesian inference\n",
        "# posterior_mean <- (prior_mean / prior_sd^2 + sum(observed_data) / n) / (1 / prior_sd^2 + 1 / n)\n",
        "# posterior_sd <- sqrt(1 / (1 / prior_sd^2 + n / 1))\n",
        "\n",
        "posterior_mean <- (prior_mean / prior_sd^2 + sum(observed_data) / sd(observed_data)^2) / (1 / prior_sd^2 + n/(sd(observed_data)^2))\n",
        "posterior_sd <- sqrt(1 / (1 / prior_sd^2 + n/(sd(observed_data)^2)))\n",
        "\n",
        "# Plotting the prior and posterior distributions\n",
        "x <- seq(prior_mean - 4 * prior_sd, prior_mean + 4 * prior_sd, length.out = 100)\n",
        "prior_density <- dnorm(x, mean = prior_mean, sd = prior_sd)\n",
        "posterior_density <- dnorm(x, mean = posterior_mean, sd = posterior_sd)\n",
        "\n",
        "prior_df <- data.frame(x, y = prior_density)\n",
        "posterior_df <- data.frame(x, y = posterior_density)\n",
        "\n",
        "p <- ggplot() +\n",
        "  geom_line(data = prior_df, aes(x, y), color = \"blue\", linetype = \"dashed\", lwd = 1) +\n",
        "  geom_line(data = posterior_df, aes(x, y), color = \"red\", lwd = 1) +\n",
        "  xlim(prior_mean - 4 * prior_sd, prior_mean + 4 * prior_sd) +\n",
        "  ylim(0, max(prior_density, posterior_density) + 0.05) +\n",
        "  xlab(\"Mean\") +\n",
        "  ylab(\"Density\") +\n",
        "  ggtitle(\"Prior and Posterior Distributions\")\n",
        "\n",
        "print(p)\n",
        "\n",
        "# Summary statistics\n",
        "cat(\"Posterior mean:\", posterior_mean, \"\\n\")\n",
        "cat(\"Posterior standard deviation:\", posterior_sd, \"\\n\")\n"
      ],
      "metadata": {
        "id": "oZEoHQHR69K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on Gene expression Model\n",
        "\n",
        "Consider a bacterial gene $G$ that produces mRNA $M$ at a fixed but unknown rate $\\sigma$. mRNA typically gets degraded at a fixed rate $d$ that can be measured directly; in this example, the degradation rate is measured to be about $d = 0.5/\\textrm{h}$. We want to infer the production rate $\\sigma$ by measuring gene expression in 30 cells using an smFISH experiment. After the experiments have been performed, the data looks like this:"
      ],
      "metadata": {
        "id": "89amIwpC-_Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ge <- c(20, 20, 19, 16, 15, 22, 17, 27, 17, 17, 21, 21, 16, 22, 25, 22, 23,\n",
        "            20, 21, 16, 16, 18, 16, 21, 17, 21, 25, 16, 15, 23)\n",
        "\n",
        "# Create a histogram\n",
        "hist(data_ge, breaks = 21, xlim = c(9.5, 30.5), col = \"lightblue\", border = \"black\",\n",
        "     xlab = \"mRNA\", ylab = \"Frequency\")\n",
        "\n",
        "# Set x-axis label and y-axis label\n",
        "xlabel <- expression(\"mRNA\")\n",
        "ylabel <- expression(\"Frequency\")\n",
        "mtext(side = 1, text = xlabel)\n",
        "mtext(side = 2, text = ylabel)\n",
        "\n"
      ],
      "metadata": {
        "id": "3jDIIEa7--VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us review the individual parts in Bayes' Theorem for this example.\n",
        "1. **The prior $p(\\theta)$:** Bacteria tend to be small and express only few mRNA molecules. A realistic production rate could be about $5/\\mathrm{h}$, but this depends on the gene. For our prior we choose a widely dispersed Gamma distribution with shape $1$ and mean $10$:\n",
        "\n",
        "$$ p(\\theta) = \\textrm{Gamma}(\\theta; 1, 5) $$\n",
        "\n",
        "2. **The likelihood $p(y  \\, | \\, \\theta)$:** The cells have been kept in the lab for a while, so we assume that mRNA production has reached steady state. At steady state, mRNA counts for this model are Poisson distributed with mean $\\theta / d$:\n",
        "\n",
        "$$ p(n \\, | \\, \\theta) = \\textrm{Poisson}(n; \\theta / d) $$\n",
        "\n",
        "Here $n$ is the number of mRNA measured. The total likelihood is just the product of the likelihoods for each individual cell, since we assume that each cell is independent of the others:\n",
        "\n",
        "$$ p(y\\, | \\, \\theta) = \\prod_{i=1}^{30} \\textrm{Poisson}(n_i; \\theta / d) $$\n",
        "\n",
        "3. **The posterior $p(\\theta \\, | \\, y)$:** In this example we can still compute this explicitly. This does not work for more complex models of gene expression.\n",
        "\n",
        "As before, computing the posterior requires the normalisation constant (we will not compute this today).\n",
        "\n",
        "**Important:** In the Bayesian setting we have to decide the prior *before* looking at the data."
      ],
      "metadata": {
        "id": "s0YQzBTw_gNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(ggplot2)\n",
        "library(gtools)\n",
        "\n",
        "# Set the degree value\n",
        "deg <- 0.5\n",
        "\n",
        "# Define the data\n",
        "data_ge <- c(20, 20, 19, 16, 15, 22, 17, 27, 17, 17, 21, 21, 16, 22, 25, 22, 23,\n",
        "              20, 21, 16, 16, 18, 16, 21, 17, 21, 25, 16, 15, 23)\n",
        "\n",
        "# Create the x-axis values\n",
        "xx <- seq(0, 20, 0.01)\n",
        "\n",
        "# Define the prior function\n",
        "prior_ge <- function(h) {\n",
        "  dgamma(h, shape = 1, scale = 5)\n",
        "}\n",
        "\n",
        "# Calculate the prior values\n",
        "prior_values <- prior_ge(xx)\n",
        "\n",
        "# Create a data frame for the prior\n",
        "prior_data <- data.frame(xx, prior_values)\n",
        "\n",
        "# Define the likelihood function\n",
        "likelihood_ge <- function(data, sigma) {\n",
        "  product <- 1\n",
        "  for (n in data) {\n",
        "    product <- product * dpois(n, lambda = sigma / deg)\n",
        "  }\n",
        "  return(product)\n",
        "}\n",
        "\n",
        "# Calculate the likelihood values\n",
        "likelihood_values <- likelihood_ge(data_ge, xx)\n",
        "\n",
        "# Create a data frame for the likelihood\n",
        "likelihood_data <- data.frame(xx, likelihood_values)\n",
        "\n",
        "# Calculate the unnormalized posterior values\n",
        "posterior_unnormalised_values <- likelihood_ge(data_ge, xx) * prior_ge(xx)\n",
        "\n",
        "# Create a data frame for the unnormalized posterior\n",
        "posterior_unnormalised_data <- data.frame(xx, posterior_unnormalised_values)\n",
        "\n",
        "# Plot the prior\n",
        "plot_prior <- ggplot(prior_data, aes(x = xx, y = prior_values)) +\n",
        "  geom_line(color = \"blue\") +\n",
        "  geom_area(fill = \"blue\", alpha = 0.3) +\n",
        "  labs(title = \"Prior\\n\", x = \"Production rate \", y = \"\") +\n",
        "  theme_minimal() +\n",
        "  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n",
        "\n",
        "# Plot the likelihood\n",
        "plot_likelihood <- ggplot(likelihood_data, aes(x = xx, y = likelihood_values)) +\n",
        "  geom_line(color = \"orange\") +\n",
        "  labs(title = \"Likelihood\\n\", x = \"Production rate \", y = \"\") +\n",
        "  theme_minimal() +\n",
        "  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n",
        "\n",
        "# Plot the posterior\n",
        "plot_posterior <- ggplot(posterior_unnormalised_data, aes(x = xx, y = posterior_unnormalised_values)) +\n",
        "  geom_line(color = \"green\") +\n",
        "  geom_area(fill = \"green\", alpha = 0.3) +\n",
        "  labs(title = \"Posterior\\n(Prior × Likelihood)\", x = \"Production rate \", y = \"\") +\n",
        "  theme_minimal() +\n",
        "  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +\n",
        "  scale_x_continuous(limits = c(0, 20))\n",
        "\n",
        "# Combine the plots\n",
        "plot_combined <- gridExtra::grid.arrange(plot_prior, plot_likelihood, plot_posterior,\n",
        "                                         nrow = 1, widths = c(4, 4, 4))\n",
        "\n",
        "# Display the combined plot\n",
        "print(plot_combined)\n"
      ],
      "metadata": {
        "id": "uZFwjiwzAX-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo methods\n",
        "\n",
        "A Monte Carlo method, named after a famous casino in Monaco, is any algorithm that uses randomly generated numbers to solve a numerical problem.\n",
        "\n",
        "For example, if we try to estimate the mean of a probability distribution $p(x)$, we can draw a few samples from $p(x)$ and compute their average. This will give a rough estimate of the mean, and we can make our estimate more precise by drawing more samples from $p(x)$. Note that our estimate of the mean can fluctuate randomly, even though the true mean of $p(x)$ is a deterministic quantity.\n",
        "\n",
        "The main advantage of Monte Carlo approaches is that they are often easier to implement than other methods, and they are usually flexible enough to work for many related problems with small modifications. Their main disadvantage is that the outputs are random by nature, and obtaining enough samples to obtain reliable estimates can be very time-consuming for some problems.\n",
        "\n",
        "The individual steps for a typical Monte Carlo method are:\n",
        "1. Fix a probability distribution to sample from\n",
        "2. Generate a sequence of random samples from that probability distribution\n",
        "3. Perform a deterministic computation on each sample\n",
        "4. Combine your results to arrive at the answer"
      ],
      "metadata": {
        "id": "z30v6idlSuSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example:** Computing the number $\\pi$ using Monte Carlo\n",
        "\n",
        "To demonstrate the flexibility of Monte Carlo methods we can try to compute $\\pi$ using randomness. In the figure below we see a circle with radius $r = 1$:"
      ],
      "metadata": {
        "id": "4zmf1I_7TErZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate angles in radians\n",
        "theta <- seq(0, 2 * pi, length.out = 100)\n",
        "\n",
        "# Generate coordinates of points on the circle\n",
        "x <- cos(theta)\n",
        "y <- sin(theta)\n",
        "\n",
        "# Plot the circle\n",
        "plot(x, y, type = \"l\", asp = 1, xlim = c(-1, 1), ylim = c(-1, 1),\n",
        "     xlab = \"x\", ylab = \"y\", main = \"Circle with Radius 1\")\n",
        "\n",
        "# Add axes\n",
        "abline(h = 0, v = 0, col = \"gray\", lty = \"dotted\")\n",
        "\n",
        "# Add a circle outline\n",
        "lines(x, y, col = \"blue\", lwd = 2)\n"
      ],
      "metadata": {
        "id": "oPbpb87NS_p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the formula $A = \\pi r^2$ we know that the circle has area $\\pi$. The square containing the circle has area $4$, so the circle fills out $\\pi/4$ of the square. So if we sample a point at random from the square, then with probability $\\pi/4$ it will lie in the circle.\n",
        "\n",
        "We can now estimate $\\pi$ by repeatedly sampling points from the square and counting how many of them lie in the circle. Our steps are:\n",
        "\n",
        "1. Our probability distribution is uniform on the square.\n",
        "2. Sample points from that distribution. They will cover the square evenly.\n",
        "3. For each point, check if it lies inside the unit circle.\n",
        "4. Return the fraction of points that pass the test, times $4$."
      ],
      "metadata": {
        "id": "2CRtJdDKZFLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for reproducibility\n",
        "set.seed(0)\n",
        "\n",
        "# Number of random points to generate\n",
        "num_points <- 10000\n",
        "\n",
        "# Generate random x and y coordinates within the square\n",
        "x <- runif(num_points, min = -1, max = 1)\n",
        "y <- runif(num_points, min = -1, max = 1)\n",
        "\n",
        "# Calculate the distance from the origin for each point\n",
        "dist <- sqrt(x^2 + y^2)\n",
        "\n",
        "# Check if each point falls within the quarter circle\n",
        "inside_circle <- dist <= 1\n",
        "\n",
        "# Count the number of points inside the circle\n",
        "num_inside_circle <- sum(inside_circle)\n",
        "\n",
        "# Estimate the area of the circle\n",
        "circle_area <- num_inside_circle / num_points\n",
        "\n",
        "# Approximate pi\n",
        "pi_approx <- 4 * circle_area\n",
        "\n",
        "# Print the estimated value of pi\n",
        "cat(\"Estimated value of pi:\", pi_approx, \"\\n\")\n",
        "\n",
        "# Plot the sampled points and the circle\n",
        "plot(x, y, xlim = c(-1, 1), ylim = c(-1, 1), xlab = \"x\", ylab = \"y\", main = \"Estimating Pi using Random Sampling\")\n",
        "points(x[inside_circle], y[inside_circle], col = \"blue\", pch = 16)\n",
        "points(x[!inside_circle], y[!inside_circle], col = \"red\", pch = 16)\n",
        "theta <- seq(0, 2 * pi, length.out = 100)\n",
        "circle_x <- cos(theta)\n",
        "circle_y <- sin(theta)\n",
        "lines(circle_x, circle_y, col = \"black\")\n"
      ],
      "metadata": {
        "id": "KbBYhyRYZkte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for reproducibility\n",
        "set.seed(0)\n",
        "\n",
        "# Number of points to generate\n",
        "N <- 1000000\n",
        "\n",
        "# Generate random x and y coordinates within the square\n",
        "points <- matrix(runif(2 * N, min = -1, max = 1), ncol = 2)\n",
        "\n",
        "# Check which points are inside the circle\n",
        "inside <- rowSums(points^2) <= 1\n",
        "\n",
        "# Compute a running estimate of π\n",
        "pi_est <- 4 * cumsum(inside) / seq_len(N)\n",
        "\n",
        "# Create the plot\n",
        "par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n",
        "\n",
        "# Plot the estimate vs. π\n",
        "plot(pi_est, ylim = c(3.1, 3.2), xlab = \"Number of samples\",\n",
        "     ylab = \"Estimate for π\",\n",
        "     main = \"Estimate vs. π\", type = \"l\")\n",
        "abline(h = pi, col = \"black\")\n",
        "\n",
        "# Plot the estimate error\n",
        "plot(pi_est - pi, ylim = c(-0.1, 0.1), xscale = \"log\", xlim = c(100, N),\n",
        "     xlab = \"Number of samples\", ylab = \"Estimate error\",\n",
        "     main = \"Estimate Error\", type = \"l\")\n",
        "abline(h = 0, col = \"black\")\n",
        "\n",
        "# Reset the par settings\n",
        "par(mfrow = c(1, 1))\n"
      ],
      "metadata": {
        "id": "4Ofm10dNcGY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "Given an arbitrary target distribution $\\pi$, we construct a Markov chain whose stationary distribution is $\\pi$. We can then sample from $\\pi$ by picking an initial value $X_0$ and simulating the Markov chain until we have reached the stationary distribution. We can formalise this as follows\n",
        "\n",
        "**Input:** Target distribution $\\pi(x)$.\n",
        "\n",
        "**Output:** Samples from the target distribution.\n",
        "\n",
        "**Steps:**\n",
        "1. Construct a Markov chain whose stationary distribution is $\\pi$\n",
        "2. Pick an arbitrary initial value $X_0$\n",
        "3. Simulate steps $X_1, X_2, X_3, \\ldots$ until $p(X_N) \\approx p(X_\\infty) = \\pi$\n",
        "4. Return $X_N, X_{N+1}, X_{N+2}, \\ldots$\n",
        "\n",
        "The number of steps required until $p(X_N)$ resembles $\\pi$ is called the burn-in period. The samples during the burn-in period, depending on the initial value $X_0$, will not be distributed correctly and are discarded. Once the burn-in is complete, subsequent samples will be approximately distributed according to the target distribution.\n",
        "\n",
        "**Note:** The stationary distribution is an *asymptotic* limit, ie. $p(X_n)$ only approximates the target distribution for large $n$. Our algorithm is therefore not exact, but we can make the difference between $p(X_n)$ and $\\pi$ as small as desired by picking a suitably large burn-in period.\n",
        "\n",
        "**Note:** Each sample returned from the algorithm is (approximately) sampled from $\\pi$, but the samples are not *independent*. Because we are simulating a random walk, each sample will be similar to the last sample. We may need a lot of samples to fully explore the target distribution.\n",
        "\n",
        "The main question we are left with is that of constructing the right Markov chain. There are a variety of ways to do this, but the most widespread one is the Metropolis-Hastings algorithm.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F1BBazvNcqGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCMC Example\n",
        "\n",
        "The target distribution is the distribution we want to sample from (in this case, an unnormalized distribution defined by a mathematical function). The proposal distribution is a distribution from which we generate proposed samples. Here, we use a normal distribution centered at the current sample value.\n",
        "\n",
        "We then perform the MCMC sampling by running a loop for the desired number of samples. In each iteration, we generate a proposal, calculate the acceptance ratio, and accept or reject the proposal based on the acceptance ratio and a random uniform value. We store the accepted samples in a vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vz53zXogv2jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MCMC Example\n",
        "\n",
        "# Define the target distribution (unnormalized)\n",
        "target_distribution <- function(x) {\n",
        "  return(exp(-x^2 / 2) * cos(5*x))\n",
        "}\n",
        "\n",
        "# Define the proposal distribution (normal distribution)\n",
        "proposal_distribution <- function(x, sigma) {\n",
        "  return(rnorm(1, mean = x, sd = sigma))\n",
        "}\n",
        "\n",
        "# Perform MCMC sampling\n",
        "n_samples <- 10000\n",
        "burn_in <- 1000\n",
        "sigma <- 0.5\n",
        "\n",
        "# Initialize the chain\n",
        "x <- 0\n",
        "samples <- numeric(n_samples)\n",
        "\n",
        "# Run the MCMC chain\n",
        "for (i in 1:n_samples) {\n",
        "  # Generate a proposal\n",
        "  x_proposal <- proposal_distribution(x, sigma)\n",
        "\n",
        "  # Calculate the acceptance ratio\n",
        "  acceptance_ratio <- target_distribution(x_proposal) / target_distribution(x)\n",
        "\n",
        "  # Accept or reject the proposal\n",
        "  if (runif(1) < acceptance_ratio) {\n",
        "    x <- x_proposal\n",
        "  }\n",
        "\n",
        "  # Store the sample\n",
        "  samples[i] <- x\n",
        "}\n",
        "\n",
        "# Remove burn-in samples\n",
        "samples <- samples[(burn_in + 1):n_samples]\n",
        "\n",
        "# Plot the samples\n",
        "hist(samples, breaks = \"FD\", freq = FALSE,\n",
        "     main = \"MCMC Sampling\",\n",
        "     xlab = \"x\", ylab = \"Density\")\n",
        "\n",
        "# Plot the sampled points\n",
        "points(samples, rep(0, length(samples)), col = \"red\")\n",
        "\n",
        "\n",
        "# Plot the target distribution\n",
        "curve(target_distribution, from = -4, to = 4, add = TRUE, col = \"blue\", lwd = 2, n = 1000)\n"
      ],
      "metadata": {
        "id": "h_AP--g7vyJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Metropolis-Hastings algorithm\n",
        "\n",
        "Assume we are given a target distribution $\\pi$ and *any* Markov chain with transition probabilities $p(x \\, | \\, y)$. In general, $\\pi$ will not be the stationary distribution of this Markov chain. But by tweaking the transition probabilities a little bit we can always arrange for that to be the case.\n",
        "\n",
        "**Notation:** In the following we denote the original transition properties by $q(x \\, | \\, y) = P(x \\, | \\, y)$. This notation is almost universally in the MCMC literature.\n",
        "\n",
        "Every time the particle wants to move from state $x$ to another state $y$, there is a certain chance that the particle stays at $x$. If this happens we say that the move has been rejected, otherwise it is accepted and the particle moves to $y$.\n",
        "\n",
        "Denote by $\\alpha(y \\, | \\, x)$ the probability that a move from $x$ to another state $y$ is accepted. The transition probability from $x$ to $y$ changes from  $q(y \\, | \\, x)$ to $\\alpha(y \\, | \\, x) \\cdot q(y \\, | \\, x)$. This defines another Markov chain.\n",
        "\n",
        "Our target distribution satisfies detailed balance for this Markov chain if\n",
        "\n",
        "$$ \\frac{\\pi(x)}{\\pi(y)} = \\frac{q(x \\, | \\, y) \\cdot \\alpha(x \\, | \\, y)}{q(y \\, | \\, x) \\cdot \\alpha(y \\, | \\, x)} $$\n",
        "\n",
        "We can rearrange this as\n",
        "\n",
        "$$ \\frac{\\alpha(y \\, | \\, x)}{\\alpha(x \\, | \\, y)} = \\frac{\\pi(y)}{\\pi(x)} \\cdot \\frac{q(x \\, | \\, y)}{q(y \\, | \\, x)} $$\n",
        "\n",
        "Call the right-hand side $r(y, x)$. We can choose any set of acceptance probabilities between $x$ and $y$ as long as they are between $0$ and $1$ and satisfy the equation below. The perhaps simplest way to achieve this is the following:\n",
        "* If $r(y, x) < 1$, let $\\alpha(y \\, | \\, x) = r(y, x)$ and $\\alpha(x \\, | \\, y) = 1$\n",
        "* If $r(y, x) > 1$, let $\\alpha(y \\, | \\, x) = 1$ and $\\alpha(x \\, | \\, y) = 1 / r(y, x)$\n",
        "\n",
        "This quantity is called the acceptance ratio.\n",
        "\n",
        "In pseudocode, if we start at position $x$, to perform the next step we do the following:\n",
        "1. Sample a value $y$ from $q(y \\, | \\, x)$\n",
        "2. Compute the Metropolis-Hastings ratio\n",
        "$$\\alpha = \\frac{\\pi(y)}{\\pi(x)} \\cdot \\frac{q(x \\, | \\, y)}{q(y \\, | \\, x)}$$\n",
        "3. If $\\alpha > 1$, move to $y$. Else, $\\alpha \\leq 1$, so move to $y$ with probability $\\alpha$, otherwise stay at $x$.\n",
        "\n",
        "**Note:** Since each step is subject to rejection, the Metropolis-Hastings algorithm will often stay at the same state for many steps. This duplication is expected behaviour.\n",
        "\n",
        "**Important:** The Metropolis-Hastings algorithm only uses the probability ratio $\\pi(y) / pi(x)$. We therefore do not have to know the normalizing constant of $\\pi$ as it cancels out in any case. This makes Metropolis-Hastings very useful for Bayesian inference, where we can often compute the unnormalized posterior (prior times likelihood), but not the normalization constant."
      ],
      "metadata": {
        "id": "z8lvdEW4dS2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metropolis Hastings Flipping Coins Example\n",
        "\n",
        "We can visualise the Bayesian approach using a simple coin example. In my wallet I have a coin which has a fixed probability $h$ to land heads each time it is flipped. How much can we say about $h$ by observing how the coin behaves? We flip the coin $8$ times and observe the sequence $H, H, H, H, T, H, H, H$.\n",
        "\n",
        "We can use our helper function to perform MCMC for us. We choose $X_0 = 0.5$ for our initial position, although any number between $0$ and $1$ will be fine. The width of our proposal distribution is $0.1$ - while any positive number will work in theory, very small or very large values will result in slow convergence. We already have the (unnormalized) posterior density, and finally choose a burn-in period of $1000$."
      ],
      "metadata": {
        "id": "ktWm8VAfsgGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Metropolis Hastings Example 1\n",
        "\n",
        "prior_ct <- function(h) {\n",
        "  return(dbeta(h, 4, 4))\n",
        "}\n",
        "\n",
        "likelihood_ct <- function(data, h) {\n",
        "  n_heads <- sum(data)\n",
        "  n_tails <- length(data) - n_heads\n",
        "\n",
        "  return(h^n_heads * (1-h)^n_tails)\n",
        "}\n",
        "\n",
        "posterior_unnormalized_ct <- function(h, data) {\n",
        "  return(likelihood_ct(data, h) * prior_ct(h))\n",
        "}\n",
        "\n",
        "simulate_step <- function(log_target, proposal_width, x) {\n",
        "  # Propose a new value\n",
        "  y <- x + proposal_width * rnorm(length(x))\n",
        "\n",
        "  log_alpha <- log_target(y) - log_target(x)  # The q-terms cancel\n",
        "\n",
        "  if (log_alpha > 0) {\n",
        "    return(y)  # Accept\n",
        "  }\n",
        "\n",
        "  alpha <- exp(log_alpha)\n",
        "\n",
        "  p <- runif(1)\n",
        "  if (p < alpha) {\n",
        "    return(y)  # Accept\n",
        "  } else {\n",
        "    return(x)  # Reject\n",
        "  }\n",
        "}\n",
        "\n",
        "metropolis_hastings <- function(log_target, proposal_width, nsteps, x0) {\n",
        "  ret <- matrix(0, nrow = nsteps, ncol = length(x0))\n",
        "  ret[1, ] <- x0\n",
        "\n",
        "  for (i in 2:nsteps) {\n",
        "    ret[i, ] <- simulate_step(log_target, proposal_width, ret[i - 1, ])\n",
        "  }\n",
        "\n",
        "  return(ret)\n",
        "}\n",
        "\n",
        "library(ggplot2)\n",
        "\n",
        "data_ct = c( 1, 1, 1, 1, 0, 1, 1)  # 1 for heads, 0 for tails\n",
        "n_heads = sum(data_ct)\n",
        "\n",
        "logposterior_unnormalized_ct <- function(x) {\n",
        "  h <- x[1]\n",
        "  # if (h < 0 || h > 1) {\n",
        "  #   return(-Inf)\n",
        "  # }\n",
        "\n",
        "  return(log(posterior_unnormalized_ct(h, data_ct)))\n",
        "}\n",
        "\n",
        "nsamples <- 1000\n",
        "burnin <- 1000\n",
        "post_ct_mcmc <- metropolis_hastings(logposterior_unnormalized_ct, 0.1, nsamples + burnin, c(0.5))\n",
        "post_ct_mcmc <- post_ct_mcmc[(burnin+1):length(post_ct_mcmc), 1]  # We assume a burn-in period of 100\n",
        "\n",
        "# This is the actual posterior\n",
        "post_ct_exact <- rbeta(nsamples, 4 + n_heads, 4 + length(data_ct) - n_heads)\n",
        "\n",
        "# Plotting\n",
        "#par(mfrow = c(1, 2), mar = c(4, 4, 2, 2), mgp = c(2.5, 0.8, 0))\n",
        "\n",
        "hist(post_ct_mcmc, breaks = 30, xlim = c(0, 1), ylim = c(0, 100),\n",
        "     xlab = \"Head probability h\", ylab = \"Posterior probability\",\n",
        "     main = \"MCMC vs. Exact Posterior\",\n",
        "     col = \"lightblue\", border = \"white\")\n",
        "hist(post_ct_exact, breaks = 30, xlim = c(0, 1), ylim = c(0, 100),\n",
        "     add = TRUE, col = \"lightgreen\", border = \"white\")\n",
        "\n",
        "legend(\"topright\", legend = c(\"MCMC\", \"Exact\"), fill = c(\"lightblue\", \"lightgreen\"))\n",
        "\n",
        "plot(post_ct_mcmc, type = \"l\", xlim = c(0, nsamples), ylim = c(0, 1),\n",
        "     xlab = \"Iterations\", ylab = \"Heads probability h\",\n",
        "     main = \"MCMC vs. Exact Posterior\",\n",
        "     col = \"lightblue\", lwd = 2)\n",
        "lines(post_ct_exact, col = \"lightgreen\", lwd = 2)\n"
      ],
      "metadata": {
        "id": "1ElmFgvMilJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metropolis Hastings Gene Expression Example\n",
        "\n",
        "To demonstrate the flexibility of MCMC we only have to change a few lines in this example. We only change the target density, the initial value (set to the prior mean) and in order to get somewhat faster convergence we set the proposal width to $0.3$."
      ],
      "metadata": {
        "id": "fL06Vm8KtEiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Metropolis Hastings Example 2\n",
        "\n",
        "deg <- 0.5\n",
        "\n",
        "# Define the data\n",
        "data_ge <- c(20, 20, 19, 16, 15, 22, 17, 27, 17, 17, 21, 21, 16, 22, 25, 22, 23,\n",
        "              20, 21, 16, 16, 18, 16, 21, 17, 21, 25, 16, 15, 23)\n",
        "\n",
        "prior_ge <- function(h) {\n",
        "  return(dgamma(h, shape = 1, scale = 5))\n",
        "}\n",
        "\n",
        "likelihood_ge <- function(data, sigma) {\n",
        "  ret <- 1\n",
        "  for (n in data) {\n",
        "    ret <- ret * dpois(n, lambda = sigma / deg)\n",
        "  }\n",
        "  return(ret)\n",
        "}\n",
        "\n",
        "posterior_unnormalized_ge <- function(h, data) {\n",
        "  return(likelihood_ge(data, h) * prior_ge(h))\n",
        "}\n",
        "\n",
        "logposterior_unnormalized_ge <- function(x) {\n",
        "  rho <- x[1]\n",
        "  return(log(posterior_unnormalized_ge(rho, data_ge)))\n",
        "}\n",
        "\n",
        "nsamples <- 1000\n",
        "burnin <- 1000\n",
        "post_ge_mcmc <- metropolis_hastings(logposterior_unnormalized_ge, 0.1, nsamples + burnin, c(5))\n",
        "post_ge_mcmc <- post_ge_mcmc[(burnin + 1):length(post_ge_mcmc), 1]  # We assume a burn-in period of 100\n",
        "\n",
        "# This is the actual posterior\n",
        "post_ge_exact <- rgamma(nsamples, 1 + sum(data_ge), scale = 5 * deg / (5 * length(data_ge) + 1))\n",
        "\n",
        "# Plotting\n",
        "\n",
        "hist(post_ge_mcmc, breaks = 50, xlim = c(0, 20), ylim = c(0, 120),\n",
        "     xlab = \"Production rate rho\", ylab = \"Posterior probability\",\n",
        "     main = \"MCMC vs. Exact Posterior\",\n",
        "     col = \"lightblue\", border = \"white\")\n",
        "hist(post_ge_exact, breaks = 50, xlim = c(0, 20), ylim = c(0, 120),\n",
        "     add = TRUE, col = \"lightgreen\", border = \"white\")\n",
        "\n",
        "legend(\"topleft\", legend = c(\"MCMC\", \"Exact\"), fill = c(\"lightblue\", \"lightgreen\"))\n",
        "\n",
        "plot(post_ge_mcmc, type = \"l\", xlim = c(0, nsamples), ylim = c(0, 20),\n",
        "     xlab = \"Iterations\", ylab = \"Production rate rho\",\n",
        "     main = \"MCMC vs. Exact Posterior\",\n",
        "     col = \"lightblue\", lwd = 2)\n",
        "lines(post_ge_exact, col = \"lightgreen\", lwd = 2)\n"
      ],
      "metadata": {
        "id": "L5xNGRHwtE8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Walks\n",
        "\n",
        "Graphically, a random walk can be defined as a system of states where each transition between one state to another is assigned a probability. We call these Markov Chains.\n",
        "\n",
        "The following graph is example of a weather system modeled by three states; sunny, cloudy, and rainy. Let's say it is currently sunny and we are interested in tomorrow's weather. In a one-step discrete Markov Chain, this graph tells us that there is a 60% chance that it will be sunny tomorrow, a 30% chance that it will be cloudy tomorrow, and a 10% chance that it will be rainy tomorrow.\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/MunskyGroup/UQBio2023/main/notebook_images/MC_graph.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "**Discrete Time Markov Chains (DTMC)**\n",
        "\n",
        "For a discrtete state space $S$, a Markov Chain is a sequence of random variables $X_0,X_1,\\dots$ taking values $x_0, x_1, \\dots, x_{n-1}, x_n,x,y \\in S$ such that\n",
        "\n",
        "$$P(X_{n+1}=y|X_0 = x_0, \\dots , X_{n-1},X_n = x) = P(X_{n+1}=y|X_n=x)$$\n",
        "where $n \\geq 0$. We say that the Markov Chain is at state $x_n$ at time $n$.\n",
        "\n",
        "The probability of transitioning from one state to another in one step can be written in matrix form denoted by the transition matrix $P$.\n",
        "\n",
        "\\begin{equation*}\n",
        "P =\n",
        "\\begin{pmatrix}\n",
        "0.3 & 0.5 & 0.2  \\\\\n",
        "0.1 & 0.5 & 0.4  \\\\\n",
        "0.3 & 0.1 & 0.6\n",
        "\\end{pmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "Note that the total probability, the sum of each row is equal to 1. Each entry $P_{x,y}$ denoted the probability of transitioning from state $x$ to $y$ in one step of the system.\n",
        "\n"
      ],
      "metadata": {
        "id": "WKSy6KjwfFql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transition matrix\n",
        "transition_matrix <- matrix(c(0.3, 0.1, 0.3,\n",
        "                              0.5, 0.5, 0.1,\n",
        "                              0.2, 0.4, 0.6), nrow = 3, byrow = TRUE)\n",
        "\n",
        "# Define the labels for each state\n",
        "state_labels <- c(\"C\", \"R\", \"S\")\n",
        "\n",
        "# Set the initial state\n",
        "initial_state <- \"C\"\n",
        "\n",
        "# Set the number of time steps\n",
        "num_steps <- 10\n",
        "\n",
        "# Generate the random sequence of states\n",
        "state_sequence <- c(initial_state)\n",
        "current_state <- initial_state\n",
        "\n",
        "for (i in 1:num_steps) {\n",
        "  next_state <- sample(state_labels, size = 1, prob = transition_matrix[which(state_labels == current_state), ])\n",
        "  state_sequence <- c(state_sequence, next_state)\n",
        "  current_state <- next_state\n",
        "}\n",
        "\n",
        "# Print the random sequence of states\n",
        "cat(\"Random Sequence of States:\\n\")\n",
        "cat(state_sequence, sep = \" \")\n"
      ],
      "metadata": {
        "id": "-tY3mdJ0fdKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good example of Markov Chains that you use every day is the Google Page Rank algorithm! Look this up in your free time!"
      ],
      "metadata": {
        "id": "yxdz88Zhfnbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Continuous Markov Chains (CTMC)**\n",
        "\n",
        "Given a set of states, the states will change according to an exponential random variable in addition to defining a probability of transitioning from one state to another. The time until we see a possible transition is exponentially distributed. We define the generator matrix where\n",
        "\n",
        "*   The non-diagonal entries are the probabilities of transitioning from a state to another state times the holding time ($q_i$) of the state. The holding time is exponentially distributed.\n",
        "*   The diagonal entries are chosen so that the row sums to 0.\n",
        "\n",
        "Example:\n",
        "\n",
        "Given the holding time parameters $(q_0,q_1,q_2) = (2,1,1/3)$ for state space $S = 0,1,2$, let the transition matrix be defined as\n",
        "\n",
        "\\begin{equation*}\n",
        "P =\n",
        "\\begin{pmatrix}\n",
        "0 & 1/2 & 1/2  \\\\\n",
        "1/2 & 0 & 1/2 \\\\\n",
        "0 & 1 & 0\n",
        "\\end{pmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "The generator matrix is defined as\n",
        "\n",
        "\\begin{equation*}\n",
        "Q =\n",
        "\\begin{pmatrix}\n",
        "-2 & 1 & 1  \\\\\n",
        "1/2 & -1 & 1/2 \\\\\n",
        "0 & 1/3 & -1/3\n",
        "\\end{pmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "where the entries are defined by $q_{ij} = q_i P_{ij}$.\n",
        "\n",
        "\n",
        "For Markov Chains, the future, given the present, is independent of the past."
      ],
      "metadata": {
        "id": "n4wkQsbsfrqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(Matrix)\n",
        "library(expm)\n",
        "\n",
        "# Define the transition rate matrix\n",
        "Q <- matrix(c(-0.2, 0.2,\n",
        "              0.3, -0.3), nrow = 2, byrow = TRUE)\n",
        "\n",
        "# Compute the transition probability matrix\n",
        "t <- 100\n",
        "P <- expm(Q*t)\n",
        "\n",
        "# Set the initial state probabilities\n",
        "pi0 <- c(0.5, 0.5)\n",
        "\n",
        "# Set the time duration\n",
        "t_max <- 100\n",
        "\n",
        "# Initialize variables to store time spent in each state\n",
        "time_in_state <- c(0,0)\n",
        "\n",
        "#start at time 0\n",
        "time = 0\n",
        "\n",
        " # Simulate a state trajectory for time t\n",
        "state_trajectory <- c()\n",
        "\n",
        "# Perform simulations up to time t_max\n",
        "while (time < t_max) {\n",
        "  current_state <- sample(0:1, size = 1, prob = pi0)\n",
        "  dt <- rexp(1, rate = abs(Q[current_state+1,current_state+1]))\n",
        "    time <- time + dt\n",
        "      next_state <- sample(0:1, size = 1, prob = P[current_state+1, ])\n",
        "      state_trajectory <- c(state_trajectory, next_state)\n",
        "      time_in_state[current_state+1] <- time_in_state[current_state+1] + dt\n",
        "      cat(\"Time in state\",current_state,\":\",paste(dt),\"\\n\")\n",
        "      current_state <- next_state\n",
        "\n",
        "}\n",
        "\n",
        "cat(\"Total Time in state\",0,\":\",paste(time_in_state[1]),\"\\n\")\n",
        "cat(\"Total Time in state\",1,\":\",paste(time_in_state[2]),\"\\n\")\n"
      ],
      "metadata": {
        "id": "PXa4HxFEfopD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brownian motion\n",
        "\n",
        "Brownian motion decribes randomness in continuous time and continuous space. **This is a CTMC.** In biology it is used to model microscopic particles moving in a viscous fluid. In these examples, we will model a symmetric random walks in 1D, 2D, and 3D.\n",
        "\n",
        "**Properties of Standard Brownian Motion**\n",
        "\n",
        "*  $B_0=0$\n",
        "*  Normal Distribution. For $t>0$, $B_t$ has a normal distribution with mean $0$ and variance $t$.\n",
        "*  Stationary increments. For $s,t>0$, $B_{t+s}-B_s$ has the same distribution as $B_t$.\n",
        "*  Independent increments. If $0 \\leq q < r \\leq s < t$, then $B_t-B_s$ and $B_r - B_q$ are independent random variables.\n",
        "*  Continuous Paths. The function $t → B_t$ is continuous with probability 1.\n",
        "\n",
        "\n",
        "We think of this as a random walk.\n",
        "\n"
      ],
      "metadata": {
        "id": "D9Px1MI_gB0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1D Brownian motion\n",
        "\n",
        "# Set the parameters for Brownian motion\n",
        "n <- 1000   # Number of time steps\n",
        "dt <- 0.01  # Time increment\n",
        "sigma <- 0.1  # Standard deviation of the random increments\n",
        "\n",
        "# Create an empty vector to store the positions\n",
        "positions <- numeric(n+1)\n",
        "\n",
        "# Generate the Brownian motion\n",
        "positions[1] <- 0  # Initial position\n",
        "\n",
        "for (i in 1:n) {\n",
        "  # Generate a random increment from a normal distribution\n",
        "  increment <- rnorm(1, mean = 0, sd = sigma * sqrt(dt))\n",
        "\n",
        "  # Update the position\n",
        "  positions[i+1] <- positions[i] + increment\n",
        "}\n",
        "\n",
        "# Plot the Brownian motion\n",
        "plot(seq(0, n*dt, by = dt), positions, type = \"l\", xlab = \"Time\", ylab = \"Position\", main = \"1D Brownian Motion\")\n"
      ],
      "metadata": {
        "id": "3m1D1eu-CJuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters for Brownian motion\n",
        "n <- 1000   # Number of time steps\n",
        "dt <- 0.01  # Time increment\n",
        "sigma <- 0.1  # Standard deviation of the random increments\n",
        "\n",
        "# Create a matrix to store the positions of multiple simulations\n",
        "simulations <- matrix(0, nrow = n+1, ncol = 10)\n",
        "\n",
        "# Generate multiple Brownian motion simulations\n",
        "for (sim in 1:10) {\n",
        "  positions <- numeric(n+1)  # Empty vector for each simulation\n",
        "  positions[1] <- 0  # Initial position\n",
        "\n",
        "  for (i in 1:n) {\n",
        "    # Generate a random increment from a normal distribution\n",
        "    increment <- rnorm(1, mean = 0, sd = sigma * sqrt(dt))\n",
        "\n",
        "    # Update the position\n",
        "    positions[i+1] <- positions[i] + increment\n",
        "  }\n",
        "\n",
        "  # Store the positions of the current simulation\n",
        "  simulations[, sim] <- positions\n",
        "}\n",
        "\n",
        "# Plot the multiple simulations\n",
        "time <- seq(0, n*dt, by = dt)\n",
        "plot(time, simulations[, 1], type = \"l\", col = \"blue\", xlab = \"Time\", ylab = \"Position\", main = \"10 Simulations of 1D Brownian Motion\")\n",
        "for (sim in 2:10) {\n",
        "  lines(time, simulations[, sim], col = \"blue\")\n",
        "}\n"
      ],
      "metadata": {
        "id": "k4IZvXqgLxwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2D Brownian motion\n",
        "\n",
        "# Set the parameters for Brownian motion\n",
        "n <- 1000   # Number of time steps\n",
        "dt <- 0.01  # Time increment\n",
        "sigma <- 0.1  # Standard deviation of the random increments\n",
        "\n",
        "# Create an empty matrix to store the positions\n",
        "positions <- matrix(0, nrow = n+1, ncol = 2)\n",
        "\n",
        "# Generate the Brownian motion\n",
        "positions[1, ] <- c(0, 0)  # Initial position\n",
        "\n",
        "for (i in 1:n) {\n",
        "  # Generate random increments from a normal distribution for both x and y\n",
        "  increments <- rnorm(2, mean = 0, sd = sigma * sqrt(dt))\n",
        "\n",
        "  # Update the positions\n",
        "  positions[i+1, ] <- positions[i, ] + increments\n",
        "}\n",
        "\n",
        "# Plot the Brownian motion\n",
        "plot(positions[, 1], positions[, 2], type = \"l\", xlab = \"X\", ylab = \"Y\", main = \"2D Brownian Motion\")\n"
      ],
      "metadata": {
        "id": "aoOuAJymLjvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters for Brownian motion\n",
        "n <- 1000   # Number of time steps\n",
        "dt <- 0.01  # Time increment\n",
        "sigma <- 0.1  # Standard deviation of the random increments\n",
        "\n",
        "# Create an empty matrix to store the positions\n",
        "positions <- matrix(0, nrow = n+1, ncol = 3)\n",
        "\n",
        "# Generate the Brownian motion\n",
        "positions[1, ] <- c(0, 0, 0)  # Initial position\n",
        "\n",
        "for (i in 1:n) {\n",
        "  # Generate random increments from a normal distribution for x, y, and z\n",
        "  increments <- rnorm(3, mean = 0, sd = sigma * sqrt(dt))\n",
        "\n",
        "  # Update the positions\n",
        "  positions[i+1, ] <- positions[i, ] + increments\n",
        "}\n",
        "\n",
        "# Plot the Brownian motion as a line plot\n",
        "library(plot3D)\n",
        "lines3D(positions[, 1], positions[, 2], positions[, 3], type = \"l\", col = \"blue\",\n",
        "        xlab = \"X\", ylab = \"Y\", zlab = \"Z\", main = \"3D Brownian Motion\")\n"
      ],
      "metadata": {
        "id": "bWA0Nl5cMi6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Differential Equations\n",
        "\n",
        "A classical differential equation which\n",
        "is perturbed by a random noise\n",
        "\n",
        "  * Additive Gaussian white noise\n",
        "\n",
        "Simplest example is the Ornstein-Uhlenbeck Process\n",
        "  * A diffusion process that was introduced as a model of the velocity of a particle undergoing Brownian motion\n",
        "\n",
        "## **History**\n",
        "Robert Brown (1828)\n",
        " * “A brief account of microscopical observations made in the months of June, July, August 1827, on the particles contained in the pollen of plants; and on the general existence of active molecules in organic and inorganic bodies.”\n",
        "\n",
        "Albert Einstein (1905)\n",
        "* “On the motion of small particles suspended in a stationary liquid, as required by the molecular kinetic theory of heat.”\n",
        "i.e. collisions between molecules\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/einsteinformula.PNG)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "Paul Langevin (1908)\n",
        "* “Sur la theorie du movement brownien.”\n",
        "i.e. Newtonian perspective\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/langevinformula.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "Norbert Wiener (1923)\n",
        "* “Existence of a continuous gaussian stochastic process with independent, stationary increments.”\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/wiener.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "The following code implements the Euler-Maruyama method and uses it to solve the Ornstein-Uhlenbeck process defined by\n",
        "\n",
        "$$ dY_t = \\theta (\\mu - Y_t)dt + \\sigma dW_t$$\n",
        "where $Y_0 = Y_{init}$ and $dW_t$ is random Gaussian noise.\n",
        "\n",
        "The Ornstein-Uhlenbeck process is a stationary Gauss-Markov process, which means that it is a Gaussian process, a Markov process, and is temporally homogeneous. Over time, the process tends to drift towards its mean function: such a process is called mean-reverting.\n",
        "\n",
        "The process can be considered to be a modification of the random walk in continuous time, or Wiener process, in which the properties of the process have been changed so that there is a tendency of the walk to move back towards a central location, with a greater attraction when the process is further away from the center.\n",
        "\n",
        "\n",
        "**Transport Types**\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/transport.PNG)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "**Motors and Microtubules**\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/MTmotor.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "\n",
        "This leads into Stochastic Calculus which is beyond the scope of this tutorial.\n",
        "\n"
      ],
      "metadata": {
        "id": "2QQNzUN5guk8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g5qLiAfdeFQ"
      },
      "outputs": [],
      "source": [
        "# @title Active Transport\n",
        "\n",
        "# Load Libraries\n",
        "library(devtools)\n",
        "library(RColorBrewer)\n",
        "library(ggplot2)\n",
        "library(forecast)\n",
        "library(dplyr)\n",
        "\n",
        "#Parameter Definitions\n",
        "Delta = .05  # fine time scale\n",
        "d = .008 #motor step size (small delta ) microns\n",
        "lambda = 10 #secs^-1   #rate of motor steps\n",
        "theta = 5*pi/6 # fixed angle of motor in radians\n",
        "Time = 30 #final time in seconds\n",
        "n = Time/Delta   #number of steps\n",
        "num_paths = 5\n",
        "kT = 0.0041  #pN microns (room temp)\n",
        "Diff = 1  #(kb*Temp)/gamma    # diffusivity, microns^2 / sec\n",
        "gamma = kT/Diff\n",
        "\n",
        "############################## ACTIVE #####################################\n",
        "\n",
        "#simulate the z(t): the motor steps with Poisson rate lambda\n",
        "z <- NULL\n",
        "z[1]= 0\n",
        "\n",
        "t <- NULL\n",
        "t[1]= 0\n",
        "\n",
        "for (i in 2:n) {\n",
        "  t[i] = t[i-1] + Delta\n",
        "}\n",
        "\n",
        "u_motor = d*lambda\n",
        "\n",
        "u_motorx <- NULL\n",
        "u_motory <- NULL\n",
        "\n",
        "u_motorx = cos(theta)*u_motor    #u1\n",
        "u_motory = sin(theta)*u_motor    #u2\n",
        "\n",
        "# parameters\n",
        "k_spring = .34 #spring constant piconewtons per micron\n",
        "k_hat = k_spring/gamma\n",
        "\n",
        "#simulate the movement of the center of the cargo (X(t),Y(t))\n",
        "df <- NULL\n",
        "\n",
        "for(j in 1:num_paths){\n",
        "\n",
        "  x <- NULL\n",
        "  y <- NULL\n",
        "  data <- NULL\n",
        "\n",
        "  x[1] = 0\n",
        "  y[1] = 0\n",
        "  seq.values <- seq(0,n-1, by=1)\n",
        "\n",
        "  for (i in 2:n) {\n",
        "    # letting delta go to infinity\n",
        "    x[i] = u_motorx*t[i-1] - u_motorx*(1/k_hat) + z[1] + rnorm(1,0,sd=sqrt(Diff/k_hat))\n",
        "    y[i] = u_motory*t[i-1] - u_motory*(1/k_hat) + z[1] + rnorm(1,0,sd=sqrt(Diff/k_hat))\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "  data <- data.frame(seq.values,x,y)\n",
        "\n",
        "  df[[j]] <- data.frame(x,y)   #save for plot with all trajectories\n",
        "\n",
        "  ## ALL TRACKS ZOOMED IN ON SQUARE PLOT\n",
        "\n",
        "  max_x = max(data[,2])\n",
        "  min_x = min(data[,2])\n",
        "  max_y = max(data[,3])\n",
        "  min_y = min(data[,3])\n",
        "\n",
        "  diff_x = max_x - min_x\n",
        "  diff_y = max_y - min_y\n",
        "  overall_max = max(diff_x,diff_y)\n",
        "\n",
        "  axis_length = overall_max/2 + .2\n",
        "\n",
        "  halfway_x = (min_x + max_x)/2\n",
        "  halfway_y = (min_y + max_y)/2\n",
        "\n",
        "  x_low = halfway_x-axis_length\n",
        "  x_high = halfway_x+axis_length\n",
        "  y_low = halfway_y-axis_length\n",
        "  y_high = halfway_y+axis_length\n",
        "\n",
        "  #individual path plots\n",
        "  tracking <- ggplot(data,aes(x=data[,2], y=data[,3], color=data[,1])) + geom_path() +\n",
        "    xlim(x_low, x_high) + ylim(y_low, y_high) + coord_fixed(ratio = 1) + theme(text = element_text(size=20))\n",
        "  print(tracking+scale_color_gradient(low=\"green\", high=\"red\") +\n",
        "          labs(title=paste(\"Path\",j, \"with\", n, \"Steps\"),x =\"x-position\"~(mu*m), y = \"y-position\"~(mu*m),color = \"Step Number\\n\"))\n",
        "\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Inactive Transport\n",
        "\n",
        "# Load Libraries\n",
        "library(devtools)\n",
        "library(RColorBrewer)\n",
        "library(ggplot2)\n",
        "library(forecast)\n",
        "library(dplyr)\n",
        "\n",
        "Delta = .05  # fine time scale\n",
        "lambda = 0 #secs^-1   #rate of motor steps\n",
        "Time = 30 #final time in seconds\n",
        "n = Time/Delta   #number of steps\n",
        "num_paths = 5\n",
        "kT = 0.0041  #pN microns (room temp)\n",
        "Diff = 1  #(kb*Temp)/gamma    # diffusivity, microns^2 / sec\n",
        "gamma = kT/Diff\n",
        "\n",
        "\n",
        "############################## INACTIVE #####################################\n",
        "\n",
        "\n",
        "#simulate the z(t): the motor steps with Poisson rate lambda\n",
        "z <- NULL\n",
        "z[1]= 0\n",
        "\n",
        "t <- NULL\n",
        "t[1]= 0\n",
        "\n",
        "for (i in 2:n) {\n",
        "  t[i] = t[i-1] + Delta\n",
        "}\n",
        "\n",
        "# parameters\n",
        "k_spring = .34 #spring constant piconewtons per micron\n",
        "k_hat = k_spring/gamma\n",
        "\n",
        "#simulate the movement of the center of the cargo (X(t),Y(t))\n",
        "df <- NULL\n",
        "\n",
        "for(j in 1:num_paths){\n",
        "\n",
        "  x <- NULL\n",
        "  y <- NULL\n",
        "  data <- NULL\n",
        "\n",
        "  x[1] = 0\n",
        "  y[1] = 0\n",
        "  seq.values <- seq(0,n-1, by=1)\n",
        "\n",
        "  for (i in 2:n) {\n",
        "\n",
        "    # letting delta go to infinity\n",
        "    x[i] = z[1] + rnorm(1,0,sd=sqrt(Diff/k_hat))\n",
        "\n",
        "    y[i] = z[1] + rnorm(1,0,sd=sqrt(Diff/k_hat))\n",
        "\n",
        "  }\n",
        "\n",
        "  data <- data.frame(seq.values,x,y)\n",
        "\n",
        "  df[[j]] <- data.frame(x,y)   #save for plot with all trajectories\n",
        "\n",
        "  ## ALL TRACKS ZOOMED IN ON SQUARE PLOT\n",
        "\n",
        "  max_x = max(data[,2])\n",
        "  min_x = min(data[,2])\n",
        "  max_y = max(data[,3])\n",
        "  min_y = min(data[,3])\n",
        "\n",
        "  diff_x = max_x - min_x\n",
        "  diff_y = max_y - min_y\n",
        "  overall_max = max(diff_x,diff_y)\n",
        "\n",
        "  axis_length = overall_max/2 + .2\n",
        "\n",
        "  halfway_x = (min_x + max_x)/2\n",
        "  halfway_y = (min_y + max_y)/2\n",
        "\n",
        "  x_low = halfway_x-axis_length\n",
        "  x_high = halfway_x+axis_length\n",
        "  y_low = halfway_y-axis_length\n",
        "  y_high = halfway_y+axis_length\n",
        "\n",
        "  #individual path plots\n",
        "  tracking <- ggplot(data,aes(x=data[,2], y=data[,3], color=data[,1])) + geom_path() +\n",
        "    xlim(x_low, x_high) + ylim(y_low, y_high) + coord_fixed(ratio = 1) + theme(text = element_text(size=20))\n",
        "  print(tracking+scale_color_gradient(low=\"green\", high=\"red\") +\n",
        "          labs(title=paste(\"Path\",j, \"with\", n, \"Steps\"),x =\"x-position\"~(mu*m), y = \"y-position\"~(mu*m),color = \"Step Number\\n\"))\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "XSWQfAq9WNI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Free/Diffusive Transport (Brownian Motion)\n",
        "\n",
        "# Load Libraries\n",
        "library(devtools)\n",
        "library(RColorBrewer)\n",
        "library(ggplot2)\n",
        "library(forecast)\n",
        "library(dplyr)\n",
        "library(sde)\n",
        "\n",
        "############################## FREE/Diffusive #####################################\n",
        "num_paths = 5\n",
        "Delta = .05  # fine time scale\n",
        "Time = 30 #final time in seconds\n",
        "n = Time/Delta   #number of steps\n",
        "\n",
        "#simulate the movement of the center of the cargo (X(t),Y(t))\n",
        "df <- NULL\n",
        "\n",
        "for(j in 1:num_paths){\n",
        "\n",
        "  x <- NULL\n",
        "  y <- NULL\n",
        "  data <- NULL\n",
        "\n",
        "  x[1] = 0\n",
        "  y[1] = 0\n",
        "  seq.values <- seq(0,n-1, by=1)\n",
        "\n",
        "  #brownian motion\n",
        "  x <- BM(N = n-1,t0=0,T=Time)\n",
        "  y <- BM(N = n-1,t0=0,T=Time)\n",
        "\n",
        "  data <- data.frame(seq.values,x,y)\n",
        "\n",
        "  df[[j]] <- data.frame(x,y)   #save for plot with all trajectories\n",
        "\n",
        "  ## ALL TRACKS ZOOMED IN ON SQUARE PLOT\n",
        "\n",
        "  max_x = max(data[,2])\n",
        "  min_x = min(data[,2])\n",
        "  max_y = max(data[,3])\n",
        "  min_y = min(data[,3])\n",
        "\n",
        "  diff_x = max_x - min_x\n",
        "  diff_y = max_y - min_y\n",
        "  overall_max = max(diff_x,diff_y)\n",
        "\n",
        "  axis_length = overall_max/2 + .2\n",
        "\n",
        "  halfway_x = (min_x + max_x)/2\n",
        "  halfway_y = (min_y + max_y)/2\n",
        "\n",
        "  x_low = halfway_x-axis_length\n",
        "  x_high = halfway_x+axis_length\n",
        "  y_low = halfway_y-axis_length\n",
        "  y_high = halfway_y+axis_length\n",
        "\n",
        "  #individual path plots\n",
        "  tracking <- ggplot(data,aes(x=data[,2], y=data[,3], color=data[,1])) + geom_path() +\n",
        "    xlim(x_low, x_high) + ylim(y_low, y_high) + coord_fixed(ratio = 1) + theme(text = element_text(size=20))\n",
        "  print(tracking+scale_color_gradient(low=\"green\", high=\"red\") +\n",
        "          labs(title=paste(\"Path\",j, \"with\", n, \"Steps\"),x =\"x-position\"~(mu*m), y = \"y-position\"~(mu*m),color = \"Step Number\\n\"))\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "eZLtMlp5WQRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Switching Transport (Two Active states)\n",
        "\n",
        "# Load Libraries\n",
        "library(devtools)\n",
        "library(RColorBrewer)\n",
        "library(ggplot2)\n",
        "library(forecast)\n",
        "library(dplyr)\n",
        "\n",
        "#Parameter Definitions\n",
        "Delta = .05  # fine time scale\n",
        "d = .008 #motor step size (small delta ) microns\n",
        "lambda = 10 #secs^-1   #rate of motor steps\n",
        "theta = 5*pi/6 # fixed angle of motor in radians\n",
        "Time = 30 #final time in seconds\n",
        "n = Time/Delta   #number of steps\n",
        "num_paths = 5\n",
        "kT = 0.0041  #pN microns (room temp)\n",
        "Diff = 1  #(kb*Temp)/gamma    # diffusivity, microns^2 / sec\n",
        "gamma = kT/Diff\n",
        "\n",
        "############################## SWITCH #####################################\n",
        "\n",
        "\n",
        "#simulate the z(t): the motor steps with Poisson rate lambda\n",
        "z <- NULL\n",
        "z_x <- NULL\n",
        "z_y <- NULL\n",
        "\n",
        "z[1]= 0\n",
        "\n",
        "t <- NULL\n",
        "t[1]= 0\n",
        "\n",
        "for (i in 2:n) {\n",
        "  t[i] = t[i-1] + Delta\n",
        "}\n",
        "\n",
        "u_motor = d*lambda\n",
        "\n",
        "u_motorx <- NULL\n",
        "u_motory <- NULL\n",
        "\n",
        "u_motorx = cos(theta)*u_motor    #u1\n",
        "u_motory = sin(theta)*u_motor    #u2\n",
        "\n",
        "# parameters\n",
        "k_spring = .34 #spring constant piconewtons per micron\n",
        "k_hat = k_spring/gamma\n",
        "\n",
        "#simulate the movement of the center of the cargo (X(t),Y(t))\n",
        "df <- NULL\n",
        "\n",
        "for(j in 1:num_paths){\n",
        "\n",
        "  z_x[1]= 0\n",
        "  z_y[1]= 0\n",
        "\n",
        "  u_motorx = cos(theta)*u_motor    #u1\n",
        "  u_motory = sin(theta)*u_motor    #u2\n",
        "\n",
        "  x <- NULL\n",
        "  y <- NULL\n",
        "  data <- NULL\n",
        "\n",
        "  x[1] = 0\n",
        "  y[1] = 0\n",
        "  seq.values <- seq(0,n-1, by=1)\n",
        "\n",
        "  for (i in 2:300) {\n",
        "    # letting delta go to infinity\n",
        "    x[i] = u_motorx*t[i-1] - u_motorx*(1/k_hat) + z_x[1] + rnorm(1,0,sd=sqrt(Diff/k_hat))\n",
        "    y[i] = u_motory*t[i-1] - u_motory*(1/k_hat) + z_y[1] + rnorm(1,0,sd=sqrt(Diff/k_hat))\n",
        "\n",
        "  }\n",
        "  theta_new = pi/6\n",
        "  u_motorx = cos(theta_new)*u_motor\n",
        "  u_motory = sin(theta_new)*u_motor\n",
        " # z_x[1]= x[300]\n",
        "  #z_y[1]= y[300]\n",
        "\n",
        "  for(i in 301:n){\n",
        "    x[i] = u_motorx*t[i-1] - u_motorx*(1/k_hat) + z_x[1] + rnorm(1,0,sd=sqrt(Diff/k_hat))\n",
        "    y[i] = u_motory*t[i-1] - u_motory*(1/k_hat) + z_y[1] + rnorm(1,0,sd=sqrt(Diff/k_hat))\n",
        "  }\n",
        "  x[301:n] = x[301:n]-(x[301]-x[300])\n",
        "  y[301:n] = y[301:n]-(y[301]-y[300])\n",
        "\n",
        "\n",
        "\n",
        "  data <- data.frame(seq.values,x,y)\n",
        "\n",
        "  df[[j]] <- data.frame(x,y)   #save for plot with all trajectories\n",
        "\n",
        "  ## ALL TRACKS ZOOMED IN ON SQUARE PLOT\n",
        "\n",
        "  max_x = max(data[,2])\n",
        "  min_x = min(data[,2])\n",
        "  max_y = max(data[,3])\n",
        "  min_y = min(data[,3])\n",
        "\n",
        "  diff_x = max_x - min_x\n",
        "  diff_y = max_y - min_y\n",
        "  overall_max = max(diff_x,diff_y)\n",
        "\n",
        "  axis_length = overall_max/2 + .2\n",
        "\n",
        "  halfway_x = (min_x + max_x)/2\n",
        "  halfway_y = (min_y + max_y)/2\n",
        "\n",
        "  x_low = halfway_x-axis_length\n",
        "  x_high = halfway_x+axis_length\n",
        "  y_low = halfway_y-axis_length\n",
        "  y_high = halfway_y+axis_length\n",
        "\n",
        "  #individual path plots\n",
        "  tracking <- ggplot(data,aes(x=data[,2], y=data[,3], color=data[,1])) + geom_path() +\n",
        "    xlim(x_low, x_high) + ylim(y_low, y_high) + coord_fixed(ratio = 1) + theme(text = element_text(size=20))\n",
        "  print(tracking+scale_color_gradient(low=\"green\", high=\"red\") +\n",
        "          labs(title=paste(\"Path\",j, \"with\", n, \"Steps\"),x =\"x-position\"~(mu*m), y = \"y-position\"~(mu*m),color = \"Step Number\\n\"))\n",
        "\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "vrGl6Zy0Z9HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CTMCs and Inference\n",
        "\n",
        "This example comes from the paper:\n",
        "\n",
        "\"Nathan T. Rayens, Keisha J. Cook, Scott A. McKinley, Christine K. Payne.\n",
        "Transport of lysosomes decreases in the perinuclear region: Insights from changepoint analysis. Biophysical Journal. (2022)(https://doi.org/10.1016/j.bpj.2022.02.032)\".\n",
        "\n",
        "We are interested in inferring the switching probabilities and the switch rates of the following cargo trajectory. We will use our study of Bayesian Inference, Exponential distributions, and Gamma distributions to infer these quantities.\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/CTMCPosterior_Page_1.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/CTMCPosterior_Page_2.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/CTMCPosterior_Page_3.png)](https://raw.githubusercontent.com/username/repository/main/image.png)\n",
        "\n",
        "[![My Image](https://raw.githubusercontent.com/kjcook13/AMIGAS_2023/main/images/CTMCPosterior_Page_4.png)](https://raw.githubusercontent.com/username/repository/main/image.png)"
      ],
      "metadata": {
        "id": "3DkznfjNn8ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HgMNPqZ1-MkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activity Problems**\n",
        "\n",
        "1. Generate a linear data set with intercept 2 and slope 3. Add noise with standard deviation 10. Fit a linear regression curve to the data and infer the intercept and slope. Estimate the parameters of the model. Plot the true linear regression (green), the data (blue), and the inferred linear regression (red).\n",
        "\n",
        "* Questions:\n",
        "  * What are the inferred parameters?\n",
        "  * Calculate the residuals.\n",
        "  * Plot the residuals against the predicted values.\n",
        "  * What happens when you increase/decrease the amount of noise in the Normal distribution?\n",
        "  * What happens when you change the distribution of the data? Try an Exponential distribution and a Gamma distribution. Can these be represented with a linear regression line?\n",
        "\n",
        "\n",
        "Below are some code snippets you may need to solve this problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "MPuTHZ-fZfdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model and fit the linear regression\n",
        "model <- lm(y ~ x)\n",
        "\n",
        "# Extract the estimated coefficients\n",
        "estimated_intercept <- coef(model)[1]\n",
        "estimated_slope <- coef(model)[2]\n",
        "\n",
        "# Plot the data, true regression line, and the fitted regression line\n",
        "plot(x, y, pch = 16, col = \"blue\", xlab = \"x\", ylab = \"y\", main = \"Linear Regression\")\n",
        "curve(true_intercept + true_slope * exp(x), add = TRUE, col = \"green\", lwd = 2, lty = 2, legend = \"True Line\")\n",
        "abline(model, col = \"red\", lwd = 2)\n",
        "legend(\"topleft\", legend = c(\"Data\", \"True Line\", \"Fitted Line\"), col = c(\"blue\", \"green\", \"red\"), pch = c(16, NA, NA), lwd = c(NA, 2, 2), lty = c(NA, 2, 1))\n",
        "\n",
        "# Plot the residuals against the predicted values\n",
        "plot(fitted(model), residuals, pch = 16, col = \"blue\", xlab = \"Predicted Values\", ylab = \"Residuals\", main = \"Residual Plot\")\n",
        "abline(h = 0, col = \"red\")\n"
      ],
      "metadata": {
        "id": "3C42c2764Si2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Simulate a particle moving in a 2D space. You have two target regions defined by circles. Let your particle start at a random generated position in the 2D plot and move according to Brownian Motion.\n",
        "\n",
        "* Questions:\n",
        "  * At what time step does the particle first reach one of the circles?\n",
        "  * If you run this for a large number of steps, how many times does the particle reach each circle?\n",
        "  * What happens when you change (increase/decrease) the noise associated with the Brownian motion?\n",
        "  * Can you define a Poisson distribution related to the number of times each circle is visited?\n",
        "  * What is the average time it takes for the particle to reach each circle?\n",
        "\n",
        "\n",
        "Below is a snippet of code that you may need to solve this problem."
      ],
      "metadata": {
        "id": "lYQjYhqu3PF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the movement of the particle\n",
        "particle_position <- matrix(start_position, nrow = num_steps + 1, ncol = 2)\n",
        "particle_position[1, ] <- start_position\n",
        "\n",
        "draw_circle <- function(center, radius) {\n",
        "  angles <- seq(0, 2 * pi, length.out = 100)\n",
        "  x <- center[1] + radius * cos(angles)\n",
        "  y <- center[2] + radius * sin(angles)\n",
        "  lines(x, y, col = \"blue\", lwd = 2)\n",
        "}\n",
        "draw_circle(circle1_center, circle1_radius)\n",
        "draw_circle(circle2_center, circle2_radius)\n",
        "\n",
        "# Add a legend\n",
        "legend(\"topright\", legend = c(\"Particle\", \"Circle 1\", \"Circle 2\"), col = c(\"red\", \"blue\", \"blue\"),\n",
        "lwd = c(0, 2, 2), pch = c(16, NA, NA), cex = 0.8)"
      ],
      "metadata": {
        "id": "dyw5RXHw5ErG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Download the dataset \"data_list.rds\" from the Github page and save it to this Google Colab notebook. Given the dataset, determine the distribution of the data. Then use Bayesian inference to estimate the parameters of the distribution; i.e. data_list: the mean and standard deviation\n"
      ],
      "metadata": {
        "id": "fNZo3WYt3nAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_list_read <- readRDS(\"data_list.rds\")\n",
        "print(data_list_read)"
      ],
      "metadata": {
        "id": "8m4se8Xs6mD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   Simulate a particle track switching between all 3 states (inactive, active, free). Let there be 1000 steps in your trajectory. Write a code that computes the four posterior distributions given in the CTCM model for particle tracking. Infer the parameters of this model."
      ],
      "metadata": {
        "id": "XKJqEo4i3pmb"
      }
    }
  ]
}